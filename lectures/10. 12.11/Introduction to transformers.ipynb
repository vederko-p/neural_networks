{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Подготовка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:13:21.135178Z",
     "iopub.status.busy": "2021-11-11T21:13:21.134619Z",
     "iopub.status.idle": "2021-11-11T21:13:21.155841Z",
     "shell.execute_reply": "2021-11-11T21:13:21.155304Z",
     "shell.execute_reply.started": "2021-11-11T21:13:21.135123Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека `transformers` (<a href='https://huggingface.co/transformers/'>huggingface</a>) позволяет разбить тексты выборки на специальные токены, которые используются в модели BERT. Рассмотрим мини-батч, в который попали два текста: \"I love Pixar.\", \"I don't care for Pixar.\", принадлежающие к классам 1 и 0 соответственно. Разобьем имеющиеся тексты на токены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:07:11.173250Z",
     "iopub.status.busy": "2021-11-11T21:07:11.172801Z",
     "iopub.status.idle": "2021-11-11T21:07:12.735467Z",
     "shell.execute_reply": "2021-11-11T21:07:12.734582Z",
     "shell.execute_reply.started": "2021-11-11T21:07:11.173141Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:07:12.737849Z",
     "iopub.status.busy": "2021-11-11T21:07:12.737349Z",
     "iopub.status.idle": "2021-11-11T21:07:22.073393Z",
     "shell.execute_reply": "2021-11-11T21:07:22.072588Z",
     "shell.execute_reply.started": "2021-11-11T21:07:12.737814Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели нам понадобится объект `'input_ids'`, содержащий id каждого из токенов, входящих в текст, а также объект `'attention_mask'`, содержащий индикаторные переменные, указывающие на специальные токены. Поскольку длина первого текста оказалась меньше длины второго, то он был дополнен специальным токеном `'PAD'`, которому соответствует индекс 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:08:00.880121Z",
     "iopub.status.busy": "2021-11-11T21:08:00.879820Z",
     "iopub.status.idle": "2021-11-11T21:08:00.904031Z",
     "shell.execute_reply": "2021-11-11T21:08:00.903169Z",
     "shell.execute_reply.started": "2021-11-11T21:08:00.880089Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:08:07.804843Z",
     "iopub.status.busy": "2021-11-11T21:08:07.804340Z",
     "iopub.status.idle": "2021-11-11T21:08:07.809921Z",
     "shell.execute_reply": "2021-11-11T21:08:07.809288Z",
     "shell.execute_reply.started": "2021-11-11T21:08:07.804789Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:08:21.806307Z",
     "iopub.status.busy": "2021-11-11T21:08:21.805593Z",
     "iopub.status.idle": "2021-11-11T21:08:21.812539Z",
     "shell.execute_reply": "2021-11-11T21:08:21.811518Z",
     "shell.execute_reply.started": "2021-11-11T21:08:21.806254Z"
    }
   },
   "source": [
    "С учетом специальных токенов первый текст выглядел бы следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:08:37.414455Z",
     "iopub.status.busy": "2021-11-11T21:08:37.413606Z",
     "iopub.status.idle": "2021-11-11T21:08:41.998623Z",
     "shell.execute_reply": "2021-11-11T21:08:41.997748Z",
     "shell.execute_reply.started": "2021-11-11T21:08:37.414418Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения модели классификации воспользуемся функцией `BertForSequenceClassification` и оптимизатором `Adam` (из `transformers`). Для дообучения модели BERT шаг рекомендуют выбирать небольшим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:09:52.497098Z",
     "iopub.status.busy": "2021-11-11T21:09:52.496450Z",
     "iopub.status.idle": "2021-11-11T21:09:52.539733Z",
     "shell.execute_reply": "2021-11-11T21:09:52.538903Z",
     "shell.execute_reply.started": "2021-11-11T21:09:52.497040Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:10:04.011929Z",
     "iopub.status.busy": "2021-11-11T21:10:04.011622Z",
     "iopub.status.idle": "2021-11-11T21:10:20.726533Z",
     "shell.execute_reply": "2021-11-11T21:10:20.725775Z",
     "shell.execute_reply.started": "2021-11-11T21:10:04.011888Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения модели схож с привычным обучением моделей в PyTorch. `BertForSequenceClassification` возвращает два объекта: значение функции потерь и тензор с предсказаниями для двух классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:11:28.569640Z",
     "iopub.status.busy": "2021-11-11T21:11:28.569337Z",
     "iopub.status.idle": "2021-11-11T21:11:29.689108Z",
     "shell.execute_reply": "2021-11-11T21:11:29.688327Z",
     "shell.execute_reply.started": "2021-11-11T21:11:28.569603Z"
    }
   },
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:11:40.605818Z",
     "iopub.status.busy": "2021-11-11T21:11:40.605540Z",
     "iopub.status.idle": "2021-11-11T21:11:40.624883Z",
     "shell.execute_reply": "2021-11-11T21:11:40.624072Z",
     "shell.execute_reply.started": "2021-11-11T21:11:40.605790Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучение всех слоев BERT требует значительных вычислительных ресурсов, поэтому стоит зафиксировать параметры энкодера и доообучать только последний слой (принцип transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:12:07.351415Z",
     "iopub.status.busy": "2021-11-11T21:12:07.351090Z",
     "iopub.status.idle": "2021-11-11T21:12:07.357371Z",
     "shell.execute_reply": "2021-11-11T21:12:07.356455Z",
     "shell.execute_reply.started": "2021-11-11T21:12:07.351379Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу анализа тональности текстов. Скачаем отзывы к фильмам и оценки тональности к ним (1 - позитивный отзыв, 0 - негативный)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:13:38.848098Z",
     "iopub.status.busy": "2021-11-11T21:13:38.847206Z",
     "iopub.status.idle": "2021-11-11T21:13:40.499211Z",
     "shell.execute_reply": "2021-11-11T21:13:40.498317Z",
     "shell.execute_reply.started": "2021-11-11T21:13:38.848043Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})\n",
    "df = df.sample(2000)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:13:44.048701Z",
     "iopub.status.busy": "2021-11-11T21:13:44.048377Z",
     "iopub.status.idle": "2021-11-11T21:13:44.061934Z",
     "shell.execute_reply": "2021-11-11T21:13:44.061271Z",
     "shell.execute_reply.started": "2021-11-11T21:13:44.048664Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T21:14:26.542890Z",
     "iopub.status.busy": "2021-11-11T21:14:26.542612Z",
     "iopub.status.idle": "2021-11-11T21:14:26.710063Z",
     "shell.execute_reply": "2021-11-11T21:14:26.709206Z",
     "shell.execute_reply.started": "2021-11-11T21:14:26.542860Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откройте новый ноутбук на kaggle (слева вкладка Code -> New Notebook). Через \"+ Add data\" справа найдите и добавьте набор данных IMDB Dataset of 50K Movie Reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите класс `IMDbDataset`, который на вход принимает тексты и метки класса (`texts`, `labels`). Указания:\n",
    "1. Под `__init__` преобразуйте `pandas.Series` с текстами в список текстов (`tokenizer` требует, чтобы тексты были представлены в виде списка).\n",
    "2. К полученному списку текстов примените `tokenizer` аналогично примеру выше. Получите `self.input_ids`, `self.attention_masks`, `self.labels = labels`.\n",
    "3. Метод `__len__` должен возвращать длину датасета (`len(self.labels)`).\n",
    "4. Метод `__getitem__` должен взвращать `input_ids`, `attention_masks`, и `labels` для каждого примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        # Ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # Ваш код здесь\n",
    "        pass\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Ваш код здесь\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код должен выполняться без ошибок, если всё сделано правильно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(texts, df['sentiment'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Определите модель аналогично примеру выше и перенесите её на `device`. Позже не забудьте перед обучением включить GPU в ноутбуке.\n",
    "6. Обучать будем методом `AdamW`. Установите шаг обучения равным `2e-3`.\n",
    "7. Количество эпох задайте равным 10.\n",
    "8. Обучите модель. Каждые 10 мини-батчей выводите среднее значение функции потерь по данным 10 мини-батчам. В отдельной переменной сохраняйте значение функции потерь на каждой из эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "# model = \n",
    "model.train()\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# optimizer = \n",
    "# n_epochs = \n",
    "train_len = len(train_loader)\n",
    "\n",
    "batch_losses =list()\n",
    "epoch_losses = list()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    batch_loss = 0\n",
    "    epoch_loss = 0\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.unsqueeze(0)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите графики процесса обучения `batch_losses` и `epoch_losses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
