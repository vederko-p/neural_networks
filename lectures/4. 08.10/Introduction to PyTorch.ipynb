{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение NumPy и PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы работы в PyTorch немногим отличаются от операций с массивами в NumPy. Рассмотрим следующую функцию ошибки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L=\\| y - X\\cdot \\Theta\\|^2_2+\\lambda \\|\\Theta\\|_1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную функцию можно последовательно реализовать средствами NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.608734260336121\n"
     ]
    }
   ],
   "source": [
    "y = np.random.randn(2, 1)\n",
    "X = np.random.randn(2, 5)\n",
    "theta = np.random.randn(5, 1)\n",
    "lmbda = 0.1\n",
    "\n",
    "pred = X.dot(theta)\n",
    "pred_loss = np.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * np.sum(np.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На PyTorch реализация вычисления функции $L$ выглядит аналогично, однако вместо массивов NumPy используются специальные объекты &ndash; `torch.Tensor`. Создать тензор в PyTorch можно как из простого массива, так и из массива Numpy, при этом указав тип данных, содержащихся в тензоре (по умолчанию float32).\n",
    "\n",
    "Подробнее о типах данных в PyTorch: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6087, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.Tensor(y)  # из NumPy массива\n",
    "X = torch.Tensor(X)  # из NumPy массива\n",
    "theta = torch.Tensor(theta).requires_grad_(True) # для theta понадобится вычислить градиент\n",
    "\n",
    "pred = X.matmul(theta)\n",
    "pred_loss = torch.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "torch.LongTensor\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor([1, 2, 3]).type())\n",
    "print(torch.Tensor(np.array([1, 2, 3])).type())\n",
    "print(torch.LongTensor([1, 2, 3]).type())  # int 64\n",
    "print(torch.zeros([2, 2, 3], dtype=torch.int64).type())\n",
    "\n",
    "print(torch.Tensor([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9],[10, 11, 12]]]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей градиентными методами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение моделей в PyTorch производится в цикле. Ниже приведена реализация метода градиентного спуска. Чтобы обучать модели градиентными методами необходимо помнить о том, что на каждой итерации необходимо \"обнулять градиенты\" (в примере ниже `theta.grad.zero_()`), поскольку по умолчанию PyTorch накапливает градиент, а не вычисляет его заново для текущих значений параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py:149: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred = X.matmul(theta)\n",
    "    pred_loss = torch.sum((y - pred) ** 2)\n",
    "    reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "    loss = pred_loss + reg_loss\n",
    "\n",
    "    loss.backward() # вычисляет градиент функции по указанным переменным при их текущих значениях\n",
    "    theta.data.add_(-0.1*theta.grad.data)  # методы с _ меняют исходный объект\n",
    "    # add в смысле прибавить, а не добавить элемент к списку\n",
    "    theta.grad.zero_()  # всегда обнулять градиент (иначе градиенты будут накапливаться)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижнее подчеркивание у функций перезаписывает значения переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([55., 50., 13.])\n",
      "tensor([50., 45.,  8.])\n",
      "tensor([55., 50., 13.])\n",
      "tensor([55., 50., 13.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([50, 45, 8])\n",
    "b = 5\n",
    "\n",
    "print(a.add(b))\n",
    "print(a)\n",
    "print(a.add_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование архитектуры нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации в качестве функции потерь используется, как правило, бинарная кросс-энтропия. Она реализована в PyTorch в виде двух функций: `BCELoss` и `BCEWithLogitsLoss`.\n",
    "\n",
    "Отличие данных функций состоит в том, что `BCELoss` вычисляет величину ошибки от выхода сети и требует, чтобы в архитектуре явно была указана функция активации на выходном слое. Функция `BCEWithLogitsLoss`, прежде чем вычислять величину потерь, применяет функцию активации (логистическую сигмоиду) к последнему слою, поэтому в архитектуре задавать функцию активации не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.FloatTensor(np.random.randint(0, 2, 5)) # float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Архитектура с `BCELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10),  # соидиняет слои;\n",
    "    # torch.nn.Linear(a, b) число нейронов на  (входе; выходе) ^\n",
    "    torch.nn.Sigmoid(),  # функция активации\n",
    "    torch.nn.Linear(10, 20),  # след. слой будет из 20 нейронов\n",
    "    torch.nn.Sigmoid(),  # функция активации\n",
    "    torch.nn.Linear(20, 1),  # последний слой из одного нейрона\n",
    "    torch.nn.Sigmoid() # явно задана функция активации\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "# weight_decay - регуляризация lambda*sum(theta**2) | lambda = weight_decay\n",
    "criterion = torch.nn.BCELoss()  # функция потерь\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X) # выдает вероятности\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерность pred (5,1); размерность y - (5) \n",
    "    # преобразовывает y к размерности 5\n",
    "    loss.backward()  # обратное распространение ошибки (градиенты на всех слоях)\n",
    "    optimizer.step()  # шаг градиентного метода (с учетом градиентов на весах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4241],\n",
      "        [0.4280],\n",
      "        [0.4214],\n",
      "        [0.4216],\n",
      "        [0.4218]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pred) # возвращает вероятности отношения объекта к классу 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример с `BCEWithLogitsLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1) # последний слой - линейный\n",
    ")\n",
    "\n",
    "# та же самая архиетктура, но последний слой линейных\n",
    "# в вероятности это будет преобразовывать сам BCEWithLogitsLoss()\n",
    "# сам брать сигмоиду и т.д.\n",
    "# и во всех остальных функциях потреь также\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерности для BCEWithLogitsLoss должны совпадать\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2700],\n",
      "        [-0.2515],\n",
      "        [-0.2353],\n",
      "        [-0.2629],\n",
      "        [-0.2587]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.4329],\n",
      "        [0.4374],\n",
      "        [0.4415],\n",
      "        [0.4346],\n",
      "        [0.4357]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pred) # возвращает выход последнего линейного слоя\n",
    "print(torch.sigmoid(pred)) # преобразование в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная классификация &ndash; частный случай многоклассовой классификации. Для многоклассовой классификации используется кросс-энтропия. Функция потерь `CrossEntropyLoss` также не требует указания функции активации последним слоем в архитектуре сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 2, 5)) # CrossEntropyLoss требует формата int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 2) # на выходном слое 2 нейрона\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss() # применяет softmax к последнему слою и вычисляет ошибку\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7742, 0.2258],\n",
      "        [0.7758, 0.2242],\n",
      "        [0.7742, 0.2258],\n",
      "        [0.7742, 0.2258],\n",
      "        [0.7745, 0.2255]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(pred, 1)) # преобразование в вероятности\n",
    "# функция активации применяется вручную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Случай нескольких классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 3) # 3 нейрона, соответствующие 3 классам\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# сама берет soft max и считает значение кросс-энтропии, поэтому выше\n",
    "# последний слой линейный\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2152, 0.3844, 0.4004],\n",
      "        [0.2223, 0.3777, 0.4001],\n",
      "        [0.2210, 0.3768, 0.4022],\n",
      "        [0.2237, 0.3777, 0.3987],\n",
      "        [0.2166, 0.3806, 0.4029]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(pred, 1))  # функция активации применяется вручную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому для бинарной классификации стоит выбирать функцию потерь также, чтобы была единость. То есть чтобы последний слой в архитектуре сети был линейным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оформление архитектуры в виде класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(3, 20)\n",
    "        self.layer_2 = torch.nn.Linear(20, 10)\n",
    "        self.layer_out = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        # если мы пишем через класс, то нам достаточно\n",
    "        # один раз прописать сигмоиду\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_1 = self.sigmoid(self.layer_1(inputs))\n",
    "        output_2 = self.sigmoid(self.layer_2(output_1))\n",
    "        output = self.layer_out(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  1  loss:  0.7446807622909546\n",
      "iter:  2  loss:  0.7316052913665771\n",
      "iter:  3  loss:  0.7209020853042603\n",
      "iter:  4  loss:  0.7121487259864807\n",
      "iter:  5  loss:  0.7049938440322876\n",
      "iter:  6  loss:  0.6991468071937561\n",
      "iter:  7  loss:  0.6943684220314026\n",
      "iter:  8  loss:  0.6904624104499817\n",
      "iter:  9  loss:  0.6872681975364685\n",
      "iter:  10  loss:  0.6846548914909363\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5,3))\n",
    "y = torch.FloatTensor(np.random.randint(0,2,5))\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1))\n",
    "    print('iter: ', i+1,' loss: ', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3> Упражнение</h3>\n",
    "<p></p>\n",
    "Реализовать и обучить модели из предыдущих заданий (binary_classification.csv, multiclass_classification.csv, светофор)\n",
    " <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "multy_data = pd.read_csv('data/multiclass_classification.csv')\n",
    "binary_data = pd.read_csv('data/task2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель для данных с двумя классами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BinaryNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(2, 3)\n",
    "        self.layer_out = torch.nn.Linear(3, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_1 = self.sigmoid(self.layer_1(inputs))        \n",
    "        output = self.layer_out(output_1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNeuralNetwork(\n",
      "  (layer_1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (layer_out): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BinaryNeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, yb = binary_data.values[:, :-1], binary_data.values[:, -1]\n",
    "Xb_train, Xb_test, yb_train, yb_test = Xb[:-5, :], Xb[-5:, :], yb[:-5], yb[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  1  loss:  0.2988814413547516\n",
      "iter:  2  loss:  0.2887113690376282\n",
      "iter:  3  loss:  0.27870890498161316\n",
      "iter:  4  loss:  0.2688693106174469\n",
      "iter:  5  loss:  0.2591882050037384\n",
      "iter:  6  loss:  0.2496614158153534\n",
      "iter:  7  loss:  0.2402849793434143\n",
      "iter:  8  loss:  0.23105524480342865\n",
      "iter:  9  loss:  0.2219686657190323\n",
      "iter:  10  loss:  0.213021919131279\n",
      "iter:  11  loss:  0.20421180129051208\n",
      "iter:  12  loss:  0.19553519785404205\n",
      "iter:  13  loss:  0.18698902428150177\n",
      "iter:  14  loss:  0.17857033014297485\n",
      "iter:  15  loss:  0.17027613520622253\n",
      "iter:  16  loss:  0.16210362315177917\n",
      "iter:  17  loss:  0.15404990315437317\n",
      "iter:  18  loss:  0.14611214399337769\n",
      "iter:  19  loss:  0.13828745484352112\n",
      "iter:  20  loss:  0.13057316839694977\n",
      "iter:  21  loss:  0.1229664757847786\n",
      "iter:  22  loss:  0.11546465754508972\n",
      "iter:  23  loss:  0.10806506127119064\n",
      "iter:  24  loss:  0.10076489299535751\n",
      "iter:  25  loss:  0.09356166422367096\n",
      "iter:  26  loss:  0.08645272254943848\n",
      "iter:  27  loss:  0.07943546026945114\n",
      "iter:  28  loss:  0.07250736653804779\n",
      "iter:  29  loss:  0.06566596031188965\n",
      "iter:  30  loss:  0.05890878662467003\n",
      "iter:  31  loss:  0.05223343148827553\n",
      "iter:  32  loss:  0.04563752934336662\n",
      "iter:  33  loss:  0.03911880403757095\n",
      "iter:  34  loss:  0.0326748751103878\n",
      "iter:  35  loss:  0.02630355767905712\n",
      "iter:  36  loss:  0.020002655684947968\n",
      "iter:  37  loss:  0.013769946061074734\n",
      "iter:  38  loss:  0.0076034157536923885\n",
      "iter:  39  loss:  0.0015009264461696148\n",
      "iter:  40  loss:  -0.004539495799690485\n",
      "iter:  41  loss:  -0.010519800707697868\n",
      "iter:  42  loss:  -0.01644197851419449\n",
      "iter:  43  loss:  -0.02230784296989441\n",
      "iter:  44  loss:  -0.02811928652226925\n",
      "iter:  45  loss:  -0.03387802094221115\n",
      "iter:  46  loss:  -0.03958579897880554\n",
      "iter:  47  loss:  -0.045244358479976654\n",
      "iter:  48  loss:  -0.05085534229874611\n",
      "iter:  49  loss:  -0.05642019957304001\n",
      "iter:  50  loss:  -0.06194062903523445\n",
      "iter:  51  loss:  -0.06741805374622345\n",
      "iter:  52  loss:  -0.07285390049219131\n",
      "iter:  53  loss:  -0.07824954390525818\n",
      "iter:  54  loss:  -0.08360637724399567\n",
      "iter:  55  loss:  -0.08892562985420227\n",
      "iter:  56  loss:  -0.09420854598283768\n",
      "iter:  57  loss:  -0.09945636242628098\n",
      "iter:  58  loss:  -0.10467009991407394\n",
      "iter:  59  loss:  -0.1098509207367897\n",
      "iter:  60  loss:  -0.11499984562397003\n",
      "iter:  61  loss:  -0.12011782824993134\n",
      "iter:  62  loss:  -0.12520581483840942\n",
      "iter:  63  loss:  -0.13026465475559235\n",
      "iter:  64  loss:  -0.1352953165769577\n",
      "iter:  65  loss:  -0.14029842615127563\n",
      "iter:  66  loss:  -0.14527477324008942\n",
      "iter:  67  loss:  -0.15022510290145874\n",
      "iter:  68  loss:  -0.15515008568763733\n",
      "iter:  69  loss:  -0.16005034744739532\n",
      "iter:  70  loss:  -0.1649264693260193\n",
      "iter:  71  loss:  -0.16977889835834503\n",
      "iter:  72  loss:  -0.17460834980010986\n",
      "iter:  73  loss:  -0.17941513657569885\n",
      "iter:  74  loss:  -0.18419981002807617\n",
      "iter:  75  loss:  -0.18896277248859406\n",
      "iter:  76  loss:  -0.19370445609092712\n",
      "iter:  77  loss:  -0.19842520356178284\n",
      "iter:  78  loss:  -0.20312538743019104\n",
      "iter:  79  loss:  -0.20780529081821442\n",
      "iter:  80  loss:  -0.2124653309583664\n",
      "iter:  81  loss:  -0.21710579097270966\n",
      "iter:  82  loss:  -0.2217269390821457\n",
      "iter:  83  loss:  -0.22632907330989838\n",
      "iter:  84  loss:  -0.2309124618768692\n",
      "iter:  85  loss:  -0.23547732830047607\n",
      "iter:  86  loss:  -0.24002394080162048\n",
      "iter:  87  loss:  -0.2445526123046875\n",
      "iter:  88  loss:  -0.24906346201896667\n",
      "iter:  89  loss:  -0.2535567879676819\n",
      "iter:  90  loss:  -0.25803279876708984\n",
      "iter:  91  loss:  -0.26249173283576965\n",
      "iter:  92  loss:  -0.2669336795806885\n",
      "iter:  93  loss:  -0.27135905623435974\n",
      "iter:  94  loss:  -0.275767982006073\n",
      "iter:  95  loss:  -0.2801606357097626\n",
      "iter:  96  loss:  -0.2845372259616852\n",
      "iter:  97  loss:  -0.28889793157577515\n",
      "iter:  98  loss:  -0.2932431101799011\n",
      "iter:  99  loss:  -0.2975727617740631\n",
      "iter:  100  loss:  -0.301887184381485\n",
      "iter:  101  loss:  -0.3061864972114563\n",
      "iter:  102  loss:  -0.3104710578918457\n",
      "iter:  103  loss:  -0.3147408366203308\n",
      "iter:  104  loss:  -0.31899625062942505\n",
      "iter:  105  loss:  -0.32323727011680603\n",
      "iter:  106  loss:  -0.32746422290802\n",
      "iter:  107  loss:  -0.33167728781700134\n",
      "iter:  108  loss:  -0.3358766436576843\n",
      "iter:  109  loss:  -0.34006252884864807\n",
      "iter:  110  loss:  -0.3442349135875702\n",
      "iter:  111  loss:  -0.3483941853046417\n",
      "iter:  112  loss:  -0.3525404930114746\n",
      "iter:  113  loss:  -0.3566740155220032\n",
      "iter:  114  loss:  -0.3607948422431946\n",
      "iter:  115  loss:  -0.36490315198898315\n",
      "iter:  116  loss:  -0.368999183177948\n",
      "iter:  117  loss:  -0.3730831444263458\n",
      "iter:  118  loss:  -0.3771551251411438\n",
      "iter:  119  loss:  -0.38121524453163147\n",
      "iter:  120  loss:  -0.3852638304233551\n",
      "iter:  121  loss:  -0.3893009126186371\n",
      "iter:  122  loss:  -0.39332664012908936\n",
      "iter:  123  loss:  -0.397341251373291\n",
      "iter:  124  loss:  -0.40134483575820923\n",
      "iter:  125  loss:  -0.40533751249313354\n",
      "iter:  126  loss:  -0.40931957960128784\n",
      "iter:  127  loss:  -0.4132910668849945\n",
      "iter:  128  loss:  -0.4172520637512207\n",
      "iter:  129  loss:  -0.4212028384208679\n",
      "iter:  130  loss:  -0.4251435697078705\n",
      "iter:  131  loss:  -0.4290742576122284\n",
      "iter:  132  loss:  -0.4329950213432312\n",
      "iter:  133  loss:  -0.4369061589241028\n",
      "iter:  134  loss:  -0.44080767035484314\n",
      "iter:  135  loss:  -0.44469964504241943\n",
      "iter:  136  loss:  -0.44858232140541077\n",
      "iter:  137  loss:  -0.4524558484554291\n",
      "iter:  138  loss:  -0.45632022619247437\n",
      "iter:  139  loss:  -0.4601757228374481\n",
      "iter:  140  loss:  -0.46402227878570557\n",
      "iter:  141  loss:  -0.46786025166511536\n",
      "iter:  142  loss:  -0.4716895520687103\n",
      "iter:  143  loss:  -0.47551026940345764\n",
      "iter:  144  loss:  -0.479322612285614\n",
      "iter:  145  loss:  -0.4831268787384033\n",
      "iter:  146  loss:  -0.4869227707386017\n",
      "iter:  147  loss:  -0.49071067571640015\n",
      "iter:  148  loss:  -0.49449074268341064\n",
      "iter:  149  loss:  -0.49826282262802124\n",
      "iter:  150  loss:  -0.5020270943641663\n",
      "iter:  151  loss:  -0.50578373670578\n",
      "iter:  152  loss:  -0.5095329880714417\n",
      "iter:  153  loss:  -0.5132747292518616\n",
      "iter:  154  loss:  -0.5170090198516846\n",
      "iter:  155  loss:  -0.5207361578941345\n",
      "iter:  156  loss:  -0.5244560241699219\n",
      "iter:  157  loss:  -0.5281689167022705\n",
      "iter:  158  loss:  -0.5318747162818909\n",
      "iter:  159  loss:  -0.5355737209320068\n",
      "iter:  160  loss:  -0.5392656922340393\n",
      "iter:  161  loss:  -0.5429511666297913\n",
      "iter:  162  loss:  -0.5466299057006836\n",
      "iter:  163  loss:  -0.5503021478652954\n",
      "iter:  164  loss:  -0.5539677739143372\n",
      "iter:  165  loss:  -0.5576270818710327\n",
      "iter:  166  loss:  -0.5612800121307373\n",
      "iter:  167  loss:  -0.5649267435073853\n",
      "iter:  168  loss:  -0.5685672760009766\n",
      "iter:  169  loss:  -0.5722018480300903\n",
      "iter:  170  loss:  -0.5758302211761475\n",
      "iter:  171  loss:  -0.5794528126716614\n",
      "iter:  172  loss:  -0.5830694437026978\n",
      "iter:  173  loss:  -0.5866802334785461\n",
      "iter:  174  loss:  -0.5902853608131409\n",
      "iter:  175  loss:  -0.5938848257064819\n",
      "iter:  176  loss:  -0.5974786281585693\n",
      "iter:  177  loss:  -0.6010669469833374\n",
      "iter:  178  loss:  -0.6046497821807861\n",
      "iter:  179  loss:  -0.6082271933555603\n",
      "iter:  180  loss:  -0.6117992997169495\n",
      "iter:  181  loss:  -0.6153661608695984\n",
      "iter:  182  loss:  -0.6189278364181519\n",
      "iter:  183  loss:  -0.6224843263626099\n",
      "iter:  184  loss:  -0.6260356903076172\n",
      "iter:  185  loss:  -0.6295821070671082\n",
      "iter:  186  loss:  -0.6331235766410828\n",
      "iter:  187  loss:  -0.6366600394248962\n",
      "iter:  188  loss:  -0.6401917934417725\n",
      "iter:  189  loss:  -0.6437184810638428\n",
      "iter:  190  loss:  -0.6472407579421997\n",
      "iter:  191  loss:  -0.6507580876350403\n",
      "iter:  192  loss:  -0.6542709469795227\n",
      "iter:  193  loss:  -0.6577792763710022\n",
      "iter:  194  loss:  -0.6612827777862549\n",
      "iter:  195  loss:  -0.6647821068763733\n",
      "iter:  196  loss:  -0.6682769060134888\n",
      "iter:  197  loss:  -0.6717674136161804\n",
      "iter:  198  loss:  -0.6752535700798035\n",
      "iter:  199  loss:  -0.6787353754043579\n",
      "iter:  200  loss:  -0.6822130084037781\n",
      "iter:  201  loss:  -0.6856865286827087\n",
      "iter:  202  loss:  -0.6891558766365051\n",
      "iter:  203  loss:  -0.6926212310791016\n",
      "iter:  204  loss:  -0.6960822939872742\n",
      "iter:  205  loss:  -0.6995396018028259\n",
      "iter:  206  loss:  -0.7029929757118225\n",
      "iter:  207  loss:  -0.7064422965049744\n",
      "iter:  208  loss:  -0.7098878622055054\n",
      "iter:  209  loss:  -0.7133297324180603\n",
      "iter:  210  loss:  -0.7167677283287048\n",
      "iter:  211  loss:  -0.7202019095420837\n",
      "iter:  212  loss:  -0.7236326336860657\n",
      "iter:  213  loss:  -0.727059543132782\n",
      "iter:  214  loss:  -0.7304829359054565\n",
      "iter:  215  loss:  -0.7339027523994446\n",
      "iter:  216  loss:  -0.7373190522193909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  217  loss:  -0.7407318353652954\n",
      "iter:  218  loss:  -0.7441412210464478\n",
      "iter:  219  loss:  -0.7475472092628479\n",
      "iter:  220  loss:  -0.7509498000144958\n",
      "iter:  221  loss:  -0.7543491125106812\n",
      "iter:  222  loss:  -0.757745087146759\n",
      "iter:  223  loss:  -0.761137843132019\n",
      "iter:  224  loss:  -0.7645274996757507\n",
      "iter:  225  loss:  -0.7679139375686646\n",
      "iter:  226  loss:  -0.7712970972061157\n",
      "iter:  227  loss:  -0.7746772766113281\n",
      "iter:  228  loss:  -0.7780542373657227\n",
      "iter:  229  loss:  -0.7814283967018127\n",
      "iter:  230  loss:  -0.7847993969917297\n",
      "iter:  231  loss:  -0.788167417049408\n",
      "iter:  232  loss:  -0.791532576084137\n",
      "iter:  233  loss:  -0.794894814491272\n",
      "iter:  234  loss:  -0.7982542514801025\n",
      "iter:  235  loss:  -0.8016107082366943\n",
      "iter:  236  loss:  -0.8049644827842712\n",
      "iter:  237  loss:  -0.8083153963088989\n",
      "iter:  238  loss:  -0.8116637468338013\n",
      "iter:  239  loss:  -0.8150091767311096\n",
      "iter:  240  loss:  -0.8183520436286926\n",
      "iter:  241  loss:  -0.821692168712616\n",
      "iter:  242  loss:  -0.8250297904014587\n",
      "iter:  243  loss:  -0.8283646702766418\n",
      "iter:  244  loss:  -0.8316971063613892\n",
      "iter:  245  loss:  -0.8350271582603455\n",
      "iter:  246  loss:  -0.8383544683456421\n",
      "iter:  247  loss:  -0.8416794538497925\n",
      "iter:  248  loss:  -0.8450018763542175\n",
      "iter:  249  loss:  -0.8483218550682068\n",
      "iter:  250  loss:  -0.8516395688056946\n",
      "iter:  251  loss:  -0.8549548983573914\n",
      "iter:  252  loss:  -0.8582678437232971\n",
      "iter:  253  loss:  -0.8615785241127014\n",
      "iter:  254  loss:  -0.8648871183395386\n",
      "iter:  255  loss:  -0.8681932091712952\n",
      "iter:  256  loss:  -0.8714971542358398\n",
      "iter:  257  loss:  -0.8747988343238831\n",
      "iter:  258  loss:  -0.8780984282493591\n",
      "iter:  259  loss:  -0.8813959360122681\n",
      "iter:  260  loss:  -0.884691059589386\n",
      "iter:  261  loss:  -0.8879842758178711\n",
      "iter:  262  loss:  -0.8912753462791443\n",
      "iter:  263  loss:  -0.8945642709732056\n",
      "iter:  264  loss:  -0.8978514075279236\n",
      "iter:  265  loss:  -0.9011364579200745\n",
      "iter:  266  loss:  -0.9044195413589478\n",
      "iter:  267  loss:  -0.9077004790306091\n",
      "iter:  268  loss:  -0.9109796285629272\n",
      "iter:  269  loss:  -0.9142569899559021\n",
      "iter:  270  loss:  -0.9175324440002441\n",
      "iter:  271  loss:  -0.9208059310913086\n",
      "iter:  272  loss:  -0.924077570438385\n",
      "iter:  273  loss:  -0.9273473024368286\n",
      "iter:  274  loss:  -0.9306156039237976\n",
      "iter:  275  loss:  -0.9338818192481995\n",
      "iter:  276  loss:  -0.9371463656425476\n",
      "iter:  277  loss:  -0.9404090046882629\n",
      "iter:  278  loss:  -0.9436703324317932\n",
      "iter:  279  loss:  -0.9469295740127563\n",
      "iter:  280  loss:  -0.9501872062683105\n",
      "iter:  281  loss:  -0.9534434080123901\n",
      "iter:  282  loss:  -0.9566978812217712\n",
      "iter:  283  loss:  -0.9599506855010986\n",
      "iter:  284  loss:  -0.9632019996643066\n",
      "iter:  285  loss:  -0.9664517045021057\n",
      "iter:  286  loss:  -0.9696997404098511\n",
      "iter:  287  loss:  -0.9729464650154114\n",
      "iter:  288  loss:  -0.9761916399002075\n",
      "iter:  289  loss:  -0.9794353246688843\n",
      "iter:  290  loss:  -0.9826774001121521\n",
      "iter:  291  loss:  -0.9859179854393005\n",
      "iter:  292  loss:  -0.9891573786735535\n",
      "iter:  293  loss:  -0.9923953413963318\n",
      "iter:  294  loss:  -0.9956315159797668\n",
      "iter:  295  loss:  -0.9988667964935303\n",
      "iter:  296  loss:  -1.0021005868911743\n",
      "iter:  297  loss:  -1.0053330659866333\n",
      "iter:  298  loss:  -1.0085641145706177\n",
      "iter:  299  loss:  -1.011793851852417\n",
      "iter:  300  loss:  -1.0150223970413208\n",
      "iter:  301  loss:  -1.018249750137329\n",
      "iter:  302  loss:  -1.0214756727218628\n",
      "iter:  303  loss:  -1.0247005224227905\n",
      "iter:  304  loss:  -1.0279239416122437\n",
      "iter:  305  loss:  -1.0311464071273804\n",
      "iter:  306  loss:  -1.0343674421310425\n",
      "iter:  307  loss:  -1.0375874042510986\n",
      "iter:  308  loss:  -1.0408061742782593\n",
      "iter:  309  loss:  -1.044024109840393\n",
      "iter:  310  loss:  -1.0472406148910522\n",
      "iter:  311  loss:  -1.050456166267395\n",
      "iter:  312  loss:  -1.0536704063415527\n",
      "iter:  313  loss:  -1.0568840503692627\n",
      "iter:  314  loss:  -1.0600961446762085\n",
      "iter:  315  loss:  -1.0633074045181274\n",
      "iter:  316  loss:  -1.0665175914764404\n",
      "iter:  317  loss:  -1.069726824760437\n",
      "iter:  318  loss:  -1.072934865951538\n",
      "iter:  319  loss:  -1.0761423110961914\n",
      "iter:  320  loss:  -1.0793484449386597\n",
      "iter:  321  loss:  -1.082553744316101\n",
      "iter:  322  loss:  -1.0857582092285156\n",
      "iter:  323  loss:  -1.0889616012573242\n",
      "iter:  324  loss:  -1.0921642780303955\n",
      "iter:  325  loss:  -1.0953660011291504\n",
      "iter:  326  loss:  -1.0985667705535889\n",
      "iter:  327  loss:  -1.1017667055130005\n",
      "iter:  328  loss:  -1.1049658060073853\n",
      "iter:  329  loss:  -1.1081639528274536\n",
      "iter:  330  loss:  -1.1113613843917847\n",
      "iter:  331  loss:  -1.1145579814910889\n",
      "iter:  332  loss:  -1.1177539825439453\n",
      "iter:  333  loss:  -1.1209489107131958\n",
      "iter:  334  loss:  -1.1241432428359985\n",
      "iter:  335  loss:  -1.1273367404937744\n",
      "iter:  336  loss:  -1.130529522895813\n",
      "iter:  337  loss:  -1.1337215900421143\n",
      "iter:  338  loss:  -1.1369128227233887\n",
      "iter:  339  loss:  -1.1401034593582153\n",
      "iter:  340  loss:  -1.1432934999465942\n",
      "iter:  341  loss:  -1.1464825868606567\n",
      "iter:  342  loss:  -1.1496713161468506\n",
      "iter:  343  loss:  -1.1528592109680176\n",
      "iter:  344  loss:  -1.1560463905334473\n",
      "iter:  345  loss:  -1.1592333316802979\n",
      "iter:  346  loss:  -1.162419080734253\n",
      "iter:  347  loss:  -1.165604591369629\n",
      "iter:  348  loss:  -1.1687896251678467\n",
      "iter:  349  loss:  -1.1719739437103271\n",
      "iter:  350  loss:  -1.1751576662063599\n",
      "iter:  351  loss:  -1.1783409118652344\n",
      "iter:  352  loss:  -1.1815234422683716\n",
      "iter:  353  loss:  -1.1847054958343506\n",
      "iter:  354  loss:  -1.1878870725631714\n",
      "iter:  355  loss:  -1.1910680532455444\n",
      "iter:  356  loss:  -1.1942487955093384\n",
      "iter:  357  loss:  -1.197428822517395\n",
      "iter:  358  loss:  -1.200608253479004\n",
      "iter:  359  loss:  -1.2037874460220337\n",
      "iter:  360  loss:  -1.2069661617279053\n",
      "iter:  361  loss:  -1.2101444005966187\n",
      "iter:  362  loss:  -1.2133222818374634\n",
      "iter:  363  loss:  -1.2164995670318604\n",
      "iter:  364  loss:  -1.2196763753890991\n",
      "iter:  365  loss:  -1.2228530645370483\n",
      "iter:  366  loss:  -1.2260290384292603\n",
      "iter:  367  loss:  -1.229204535484314\n",
      "iter:  368  loss:  -1.2323799133300781\n",
      "iter:  369  loss:  -1.2355551719665527\n",
      "iter:  370  loss:  -1.23872971534729\n",
      "iter:  371  loss:  -1.2419041395187378\n",
      "iter:  372  loss:  -1.2450780868530273\n",
      "iter:  373  loss:  -1.2482517957687378\n",
      "iter:  374  loss:  -1.25142502784729\n",
      "iter:  375  loss:  -1.2545980215072632\n",
      "iter:  376  loss:  -1.2577708959579468\n",
      "iter:  377  loss:  -1.2609434127807617\n",
      "iter:  378  loss:  -1.2641156911849976\n",
      "iter:  379  loss:  -1.2672874927520752\n",
      "iter:  380  loss:  -1.2704591751098633\n",
      "iter:  381  loss:  -1.2736306190490723\n",
      "iter:  382  loss:  -1.2768018245697021\n",
      "iter:  383  loss:  -1.2799726724624634\n",
      "iter:  384  loss:  -1.2831436395645142\n",
      "iter:  385  loss:  -1.2863140106201172\n",
      "iter:  386  loss:  -1.2894843816757202\n",
      "iter:  387  loss:  -1.2926546335220337\n",
      "iter:  388  loss:  -1.295824408531189\n",
      "iter:  389  loss:  -1.2989941835403442\n",
      "iter:  390  loss:  -1.3021636009216309\n",
      "iter:  391  loss:  -1.3053334951400757\n",
      "iter:  392  loss:  -1.3085027933120728\n",
      "iter:  393  loss:  -1.3116717338562012\n",
      "iter:  394  loss:  -1.3148409128189087\n",
      "iter:  395  loss:  -1.3180099725723267\n",
      "iter:  396  loss:  -1.3211787939071655\n",
      "iter:  397  loss:  -1.3243474960327148\n",
      "iter:  398  loss:  -1.3275160789489746\n",
      "iter:  399  loss:  -1.3306846618652344\n",
      "iter:  400  loss:  -1.3338528871536255\n",
      "iter:  401  loss:  -1.3370213508605957\n",
      "iter:  402  loss:  -1.3401895761489868\n",
      "iter:  403  loss:  -1.3433579206466675\n",
      "iter:  404  loss:  -1.3465261459350586\n",
      "iter:  405  loss:  -1.3496944904327393\n",
      "iter:  406  loss:  -1.3528623580932617\n",
      "iter:  407  loss:  -1.3560304641723633\n",
      "iter:  408  loss:  -1.3591984510421753\n",
      "iter:  409  loss:  -1.3623664379119873\n",
      "iter:  410  loss:  -1.365534782409668\n",
      "iter:  411  loss:  -1.368703007698059\n",
      "iter:  412  loss:  -1.3718706369400024\n",
      "iter:  413  loss:  -1.375038981437683\n",
      "iter:  414  loss:  -1.3782070875167847\n",
      "iter:  415  loss:  -1.3813754320144653\n",
      "iter:  416  loss:  -1.3845436573028564\n",
      "iter:  417  loss:  -1.387712001800537\n",
      "iter:  418  loss:  -1.3908801078796387\n",
      "iter:  419  loss:  -1.3940485715866089\n",
      "iter:  420  loss:  -1.3972171545028687\n",
      "iter:  421  loss:  -1.4003857374191284\n",
      "iter:  422  loss:  -1.4035545587539673\n",
      "iter:  423  loss:  -1.406723141670227\n",
      "iter:  424  loss:  -1.4098918437957764\n",
      "iter:  425  loss:  -1.4130610227584839\n",
      "iter:  426  loss:  -1.4162299633026123\n",
      "iter:  427  loss:  -1.419399380683899\n",
      "iter:  428  loss:  -1.4225685596466064\n",
      "iter:  429  loss:  -1.4257380962371826\n",
      "iter:  430  loss:  -1.4289073944091797\n",
      "iter:  431  loss:  -1.4320772886276245\n",
      "iter:  432  loss:  -1.4352471828460693\n",
      "iter:  433  loss:  -1.4384171962738037\n",
      "iter:  434  loss:  -1.4415875673294067\n",
      "iter:  435  loss:  -1.4447580575942993\n",
      "iter:  436  loss:  -1.4479286670684814\n",
      "iter:  437  loss:  -1.451099157333374\n",
      "iter:  438  loss:  -1.4542701244354248\n",
      "iter:  439  loss:  -1.4574414491653442\n",
      "iter:  440  loss:  -1.4606125354766846\n",
      "iter:  441  loss:  -1.4637843370437622\n",
      "iter:  442  loss:  -1.4669561386108398\n",
      "iter:  443  loss:  -1.4701281785964966\n",
      "iter:  444  loss:  -1.4733004570007324\n",
      "iter:  445  loss:  -1.4764727354049683\n",
      "iter:  446  loss:  -1.4796454906463623\n",
      "iter:  447  loss:  -1.4828184843063354\n",
      "iter:  448  loss:  -1.4859917163848877\n",
      "iter:  449  loss:  -1.4891653060913086\n",
      "iter:  450  loss:  -1.4923391342163086\n",
      "iter:  451  loss:  -1.4955132007598877\n",
      "iter:  452  loss:  -1.498687505722046\n",
      "iter:  453  loss:  -1.5018621683120728\n",
      "iter:  454  loss:  -1.5050371885299683\n",
      "iter:  455  loss:  -1.5082125663757324\n",
      "iter:  456  loss:  -1.5113880634307861\n",
      "iter:  457  loss:  -1.5145636796951294\n",
      "iter:  458  loss:  -1.51774001121521\n",
      "iter:  459  loss:  -1.5209163427352905\n",
      "iter:  460  loss:  -1.5240932703018188\n",
      "iter:  461  loss:  -1.5272703170776367\n",
      "iter:  462  loss:  -1.5304476022720337\n",
      "iter:  463  loss:  -1.5336254835128784\n",
      "iter:  464  loss:  -1.5368038415908813\n",
      "iter:  465  loss:  -1.5399824380874634\n",
      "iter:  466  loss:  -1.5431612730026245\n",
      "iter:  467  loss:  -1.5463405847549438\n",
      "iter:  468  loss:  -1.5495201349258423\n",
      "iter:  469  loss:  -1.5527005195617676\n",
      "iter:  470  loss:  -1.5558807849884033\n",
      "iter:  471  loss:  -1.5590617656707764\n",
      "iter:  472  loss:  -1.562243103981018\n",
      "iter:  473  loss:  -1.5654246807098389\n",
      "iter:  474  loss:  -1.5686066150665283\n",
      "iter:  475  loss:  -1.5717891454696655\n",
      "iter:  476  loss:  -1.5749719142913818\n",
      "iter:  477  loss:  -1.578155279159546\n",
      "iter:  478  loss:  -1.5813390016555786\n",
      "iter:  479  loss:  -1.584523320198059\n",
      "iter:  480  loss:  -1.587707757949829\n",
      "iter:  481  loss:  -1.5908927917480469\n",
      "iter:  482  loss:  -1.5940783023834229\n",
      "iter:  483  loss:  -1.5972644090652466\n",
      "iter:  484  loss:  -1.6004509925842285\n",
      "iter:  485  loss:  -1.6036375761032104\n",
      "iter:  486  loss:  -1.6068249940872192\n",
      "iter:  487  loss:  -1.6100130081176758\n",
      "iter:  488  loss:  -1.613201379776001\n",
      "iter:  489  loss:  -1.6163899898529053\n",
      "iter:  490  loss:  -1.619579553604126\n",
      "iter:  491  loss:  -1.6227693557739258\n",
      "iter:  492  loss:  -1.6259599924087524\n",
      "iter:  493  loss:  -1.6291507482528687\n",
      "iter:  494  loss:  -1.632341980934143\n",
      "iter:  495  loss:  -1.6355339288711548\n",
      "iter:  496  loss:  -1.6387265920639038\n",
      "iter:  497  loss:  -1.641919493675232\n",
      "iter:  498  loss:  -1.6451126337051392\n",
      "iter:  499  loss:  -1.648306965827942\n",
      "iter:  500  loss:  -1.6515014171600342\n",
      "iter:  501  loss:  -1.6546964645385742\n",
      "iter:  502  loss:  -1.6578923463821411\n",
      "iter:  503  loss:  -1.6610885858535767\n",
      "iter:  504  loss:  -1.6642857789993286\n",
      "iter:  505  loss:  -1.6674827337265015\n",
      "iter:  506  loss:  -1.6706809997558594\n",
      "iter:  507  loss:  -1.6738795042037964\n",
      "iter:  508  loss:  -1.6770788431167603\n",
      "iter:  509  loss:  -1.6802787780761719\n",
      "iter:  510  loss:  -1.6834790706634521\n",
      "iter:  511  loss:  -1.6866798400878906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  512  loss:  -1.6898815631866455\n",
      "iter:  513  loss:  -1.6930835247039795\n",
      "iter:  514  loss:  -1.6962867975234985\n",
      "iter:  515  loss:  -1.699489951133728\n",
      "iter:  516  loss:  -1.702694296836853\n",
      "iter:  517  loss:  -1.7058990001678467\n",
      "iter:  518  loss:  -1.709104299545288\n",
      "iter:  519  loss:  -1.712310552597046\n",
      "iter:  520  loss:  -1.715517282485962\n",
      "iter:  521  loss:  -1.7187243700027466\n",
      "iter:  522  loss:  -1.7219325304031372\n",
      "iter:  523  loss:  -1.725140929222107\n",
      "iter:  524  loss:  -1.728350281715393\n",
      "iter:  525  loss:  -1.731560468673706\n",
      "iter:  526  loss:  -1.7347713708877563\n",
      "iter:  527  loss:  -1.7379823923110962\n",
      "iter:  528  loss:  -1.7411949634552002\n",
      "iter:  529  loss:  -1.7444075345993042\n",
      "iter:  530  loss:  -1.7476211786270142\n",
      "iter:  531  loss:  -1.7508351802825928\n",
      "iter:  532  loss:  -1.754050374031067\n",
      "iter:  533  loss:  -1.7572659254074097\n",
      "iter:  534  loss:  -1.7604824304580688\n",
      "iter:  535  loss:  -1.7636996507644653\n",
      "iter:  536  loss:  -1.7669175863265991\n",
      "iter:  537  loss:  -1.770135760307312\n",
      "iter:  538  loss:  -1.7733553647994995\n",
      "iter:  539  loss:  -1.7765755653381348\n",
      "iter:  540  loss:  -1.7797966003417969\n",
      "iter:  541  loss:  -1.7830182313919067\n",
      "iter:  542  loss:  -1.786240577697754\n",
      "iter:  543  loss:  -1.7894636392593384\n",
      "iter:  544  loss:  -1.7926875352859497\n",
      "iter:  545  loss:  -1.7959123849868774\n",
      "iter:  546  loss:  -1.7991379499435425\n",
      "iter:  547  loss:  -1.8023643493652344\n",
      "iter:  548  loss:  -1.8055914640426636\n",
      "iter:  549  loss:  -1.80881929397583\n",
      "iter:  550  loss:  -1.812048077583313\n",
      "iter:  551  loss:  -1.8152774572372437\n",
      "iter:  552  loss:  -1.8185081481933594\n",
      "iter:  553  loss:  -1.8217390775680542\n",
      "iter:  554  loss:  -1.824971318244934\n",
      "iter:  555  loss:  -1.8282041549682617\n",
      "iter:  556  loss:  -1.8314379453659058\n",
      "iter:  557  loss:  -1.8346725702285767\n",
      "iter:  558  loss:  -1.8379080295562744\n",
      "iter:  559  loss:  -1.8411444425582886\n",
      "iter:  560  loss:  -1.8443812131881714\n",
      "iter:  561  loss:  -1.8476194143295288\n",
      "iter:  562  loss:  -1.8508583307266235\n",
      "iter:  563  loss:  -1.8540982007980347\n",
      "iter:  564  loss:  -1.8573386669158936\n",
      "iter:  565  loss:  -1.8605804443359375\n",
      "iter:  566  loss:  -1.8638228178024292\n",
      "iter:  567  loss:  -1.8670660257339478\n",
      "iter:  568  loss:  -1.8703105449676514\n",
      "iter:  569  loss:  -1.8735557794570923\n",
      "iter:  570  loss:  -1.8768017292022705\n",
      "iter:  571  loss:  -1.8800487518310547\n",
      "iter:  572  loss:  -1.8832964897155762\n",
      "iter:  573  loss:  -1.8865456581115723\n",
      "iter:  574  loss:  -1.8897954225540161\n",
      "iter:  575  loss:  -1.893046259880066\n",
      "iter:  576  loss:  -1.8962979316711426\n",
      "iter:  577  loss:  -1.899550437927246\n",
      "iter:  578  loss:  -1.9028042554855347\n",
      "iter:  579  loss:  -1.9060587882995605\n",
      "iter:  580  loss:  -1.9093142747879028\n",
      "iter:  581  loss:  -1.912570595741272\n",
      "iter:  582  loss:  -1.9158282279968262\n",
      "iter:  583  loss:  -1.9190864562988281\n",
      "iter:  584  loss:  -1.9223462343215942\n",
      "iter:  585  loss:  -1.925606608390808\n",
      "iter:  586  loss:  -1.9288681745529175\n",
      "iter:  587  loss:  -1.9321306943893433\n",
      "iter:  588  loss:  -1.9353939294815063\n",
      "iter:  589  loss:  -1.9386587142944336\n",
      "iter:  590  loss:  -1.9419243335723877\n",
      "iter:  591  loss:  -1.945190668106079\n",
      "iter:  592  loss:  -1.9484580755233765\n",
      "iter:  593  loss:  -1.951727032661438\n",
      "iter:  594  loss:  -1.9549968242645264\n",
      "iter:  595  loss:  -1.9582675695419312\n",
      "iter:  596  loss:  -1.961539387702942\n",
      "iter:  597  loss:  -1.9648122787475586\n",
      "iter:  598  loss:  -1.9680860042572021\n",
      "iter:  599  loss:  -1.9713610410690308\n",
      "iter:  600  loss:  -1.9746370315551758\n",
      "iter:  601  loss:  -1.9779143333435059\n",
      "iter:  602  loss:  -1.981192708015442\n",
      "iter:  603  loss:  -1.9844719171524048\n",
      "iter:  604  loss:  -1.9877527952194214\n",
      "iter:  605  loss:  -1.9910337924957275\n",
      "iter:  606  loss:  -1.994316816329956\n",
      "iter:  607  loss:  -1.9976005554199219\n",
      "iter:  608  loss:  -2.000885486602783\n",
      "iter:  609  loss:  -2.004171371459961\n",
      "iter:  610  loss:  -2.007458448410034\n",
      "iter:  611  loss:  -2.010746717453003\n",
      "iter:  612  loss:  -2.014036178588867\n",
      "iter:  613  loss:  -2.017327070236206\n",
      "iter:  614  loss:  -2.0206186771392822\n",
      "iter:  615  loss:  -2.023911714553833\n",
      "iter:  616  loss:  -2.0272059440612793\n",
      "iter:  617  loss:  -2.030501127243042\n",
      "iter:  618  loss:  -2.0337977409362793\n",
      "iter:  619  loss:  -2.037095069885254\n",
      "iter:  620  loss:  -2.040393590927124\n",
      "iter:  621  loss:  -2.043693780899048\n",
      "iter:  622  loss:  -2.046995162963867\n",
      "iter:  623  loss:  -2.050297260284424\n",
      "iter:  624  loss:  -2.053600788116455\n",
      "iter:  625  loss:  -2.05690598487854\n",
      "iter:  626  loss:  -2.060211658477783\n",
      "iter:  627  loss:  -2.06351900100708\n",
      "iter:  628  loss:  -2.0668277740478516\n",
      "iter:  629  loss:  -2.0701375007629395\n",
      "iter:  630  loss:  -2.0734481811523438\n",
      "iter:  631  loss:  -2.0767602920532227\n",
      "iter:  632  loss:  -2.0800740718841553\n",
      "iter:  633  loss:  -2.0833888053894043\n",
      "iter:  634  loss:  -2.0867044925689697\n",
      "iter:  635  loss:  -2.090021848678589\n",
      "iter:  636  loss:  -2.0933403968811035\n",
      "iter:  637  loss:  -2.0966598987579346\n",
      "iter:  638  loss:  -2.0999810695648193\n",
      "iter:  639  loss:  -2.1033034324645996\n",
      "iter:  640  loss:  -2.1066274642944336\n",
      "iter:  641  loss:  -2.109952211380005\n",
      "iter:  642  loss:  -2.11327862739563\n",
      "iter:  643  loss:  -2.1166062355041504\n",
      "iter:  644  loss:  -2.1199352741241455\n",
      "iter:  645  loss:  -2.123265027999878\n",
      "iter:  646  loss:  -2.126596689224243\n",
      "iter:  647  loss:  -2.129929542541504\n",
      "iter:  648  loss:  -2.1332640647888184\n",
      "iter:  649  loss:  -2.136599540710449\n",
      "iter:  650  loss:  -2.139936685562134\n",
      "iter:  651  loss:  -2.143275022506714\n",
      "iter:  652  loss:  -2.1466143131256104\n",
      "iter:  653  loss:  -2.1499552726745605\n",
      "iter:  654  loss:  -2.1532979011535645\n",
      "iter:  655  loss:  -2.156641960144043\n",
      "iter:  656  loss:  -2.159986972808838\n",
      "iter:  657  loss:  -2.1633331775665283\n",
      "iter:  658  loss:  -2.1666815280914307\n",
      "iter:  659  loss:  -2.170030355453491\n",
      "iter:  660  loss:  -2.1733810901641846\n",
      "iter:  661  loss:  -2.1767334938049316\n",
      "iter:  662  loss:  -2.180086851119995\n",
      "iter:  663  loss:  -2.1834418773651123\n",
      "iter:  664  loss:  -2.186798334121704\n",
      "iter:  665  loss:  -2.1901562213897705\n",
      "iter:  666  loss:  -2.1935155391693115\n",
      "iter:  667  loss:  -2.196876287460327\n",
      "iter:  668  loss:  -2.2002384662628174\n",
      "iter:  669  loss:  -2.2036020755767822\n",
      "iter:  670  loss:  -2.2069671154022217\n",
      "iter:  671  loss:  -2.210333824157715\n",
      "iter:  672  loss:  -2.2137019634246826\n",
      "iter:  673  loss:  -2.217071056365967\n",
      "iter:  674  loss:  -2.220442056655884\n",
      "iter:  675  loss:  -2.2238147258758545\n",
      "iter:  676  loss:  -2.2271888256073\n",
      "iter:  677  loss:  -2.2305641174316406\n",
      "iter:  678  loss:  -2.233941078186035\n",
      "iter:  679  loss:  -2.2373197078704834\n",
      "iter:  680  loss:  -2.2406997680664062\n",
      "iter:  681  loss:  -2.2440810203552246\n",
      "iter:  682  loss:  -2.247464418411255\n",
      "iter:  683  loss:  -2.2508490085601807\n",
      "iter:  684  loss:  -2.254235029220581\n",
      "iter:  685  loss:  -2.2576229572296143\n",
      "iter:  686  loss:  -2.261011838912964\n",
      "iter:  687  loss:  -2.2644031047821045\n",
      "iter:  688  loss:  -2.2677953243255615\n",
      "iter:  689  loss:  -2.2711894512176514\n",
      "iter:  690  loss:  -2.2745847702026367\n",
      "iter:  691  loss:  -2.277981996536255\n",
      "iter:  692  loss:  -2.2813806533813477\n",
      "iter:  693  loss:  -2.284780740737915\n",
      "iter:  694  loss:  -2.2881829738616943\n",
      "iter:  695  loss:  -2.291586399078369\n",
      "iter:  696  loss:  -2.2949914932250977\n",
      "iter:  697  loss:  -2.2983977794647217\n",
      "iter:  698  loss:  -2.3018064498901367\n",
      "iter:  699  loss:  -2.3052163124084473\n",
      "iter:  700  loss:  -2.3086278438568115\n",
      "iter:  701  loss:  -2.3120410442352295\n",
      "iter:  702  loss:  -2.315455913543701\n",
      "iter:  703  loss:  -2.3188726902008057\n",
      "iter:  704  loss:  -2.3222906589508057\n",
      "iter:  705  loss:  -2.3257105350494385\n",
      "iter:  706  loss:  -2.329132080078125\n",
      "iter:  707  loss:  -2.3325555324554443\n",
      "iter:  708  loss:  -2.3359804153442383\n",
      "iter:  709  loss:  -2.339406728744507\n",
      "iter:  710  loss:  -2.342834949493408\n",
      "iter:  711  loss:  -2.3462648391723633\n",
      "iter:  712  loss:  -2.3496971130371094\n",
      "iter:  713  loss:  -2.353130578994751\n",
      "iter:  714  loss:  -2.3565659523010254\n",
      "iter:  715  loss:  -2.3600025177001953\n",
      "iter:  716  loss:  -2.363440752029419\n",
      "iter:  717  loss:  -2.3668813705444336\n",
      "iter:  718  loss:  -2.370323419570923\n",
      "iter:  719  loss:  -2.373767375946045\n",
      "iter:  720  loss:  -2.3772132396698\n",
      "iter:  721  loss:  -2.3806605339050293\n",
      "iter:  722  loss:  -2.3841097354888916\n",
      "iter:  723  loss:  -2.3875608444213867\n",
      "iter:  724  loss:  -2.3910136222839355\n",
      "iter:  725  loss:  -2.394468307495117\n",
      "iter:  726  loss:  -2.3979244232177734\n",
      "iter:  727  loss:  -2.4013826847076416\n",
      "iter:  728  loss:  -2.4048426151275635\n",
      "iter:  729  loss:  -2.4083049297332764\n",
      "iter:  730  loss:  -2.411768674850464\n",
      "iter:  731  loss:  -2.415233850479126\n",
      "iter:  732  loss:  -2.418701648712158\n",
      "iter:  733  loss:  -2.422170639038086\n",
      "iter:  734  loss:  -2.425642251968384\n",
      "iter:  735  loss:  -2.429114818572998\n",
      "iter:  736  loss:  -2.432589292526245\n",
      "iter:  737  loss:  -2.436066150665283\n",
      "iter:  738  loss:  -2.439545154571533\n",
      "iter:  739  loss:  -2.443025827407837\n",
      "iter:  740  loss:  -2.4465084075927734\n",
      "iter:  741  loss:  -2.4499928951263428\n",
      "iter:  742  loss:  -2.453479290008545\n",
      "iter:  743  loss:  -2.45696759223938\n",
      "iter:  744  loss:  -2.4604575634002686\n",
      "iter:  745  loss:  -2.4639499187469482\n",
      "iter:  746  loss:  -2.4674441814422607\n",
      "iter:  747  loss:  -2.470940589904785\n",
      "iter:  748  loss:  -2.474438190460205\n",
      "iter:  749  loss:  -2.477938413619995\n",
      "iter:  750  loss:  -2.481440782546997\n",
      "iter:  751  loss:  -2.484945058822632\n",
      "iter:  752  loss:  -2.488450765609741\n",
      "iter:  753  loss:  -2.4919590950012207\n",
      "iter:  754  loss:  -2.495469570159912\n",
      "iter:  755  loss:  -2.4989817142486572\n",
      "iter:  756  loss:  -2.5024962425231934\n",
      "iter:  757  loss:  -2.506012201309204\n",
      "iter:  758  loss:  -2.509530782699585\n",
      "iter:  759  loss:  -2.5130515098571777\n",
      "iter:  760  loss:  -2.5165743827819824\n",
      "iter:  761  loss:  -2.5200986862182617\n",
      "iter:  762  loss:  -2.523625612258911\n",
      "iter:  763  loss:  -2.5271544456481934\n",
      "iter:  764  loss:  -2.5306856632232666\n",
      "iter:  765  loss:  -2.5342187881469727\n",
      "iter:  766  loss:  -2.537754535675049\n",
      "iter:  767  loss:  -2.541292190551758\n",
      "iter:  768  loss:  -2.5448317527770996\n",
      "iter:  769  loss:  -2.5483736991882324\n",
      "iter:  770  loss:  -2.551917314529419\n",
      "iter:  771  loss:  -2.555464029312134\n",
      "iter:  772  loss:  -2.5590128898620605\n",
      "iter:  773  loss:  -2.56256365776062\n",
      "iter:  774  loss:  -2.5661165714263916\n",
      "iter:  775  loss:  -2.569671630859375\n",
      "iter:  776  loss:  -2.5732295513153076\n",
      "iter:  777  loss:  -2.576789617538452\n",
      "iter:  778  loss:  -2.5803513526916504\n",
      "iter:  779  loss:  -2.583916187286377\n",
      "iter:  780  loss:  -2.587482452392578\n",
      "iter:  781  loss:  -2.5910518169403076\n",
      "iter:  782  loss:  -2.594623327255249\n",
      "iter:  783  loss:  -2.5981969833374023\n",
      "iter:  784  loss:  -2.6017730236053467\n",
      "iter:  785  loss:  -2.605351448059082\n",
      "iter:  786  loss:  -2.6089320182800293\n",
      "iter:  787  loss:  -2.612515687942505\n",
      "iter:  788  loss:  -2.6161012649536133\n",
      "iter:  789  loss:  -2.6196892261505127\n",
      "iter:  790  loss:  -2.6232800483703613\n",
      "iter:  791  loss:  -2.6268727779388428\n",
      "iter:  792  loss:  -2.6304678916931152\n",
      "iter:  793  loss:  -2.634065628051758\n",
      "iter:  794  loss:  -2.6376662254333496\n",
      "iter:  795  loss:  -2.641268730163574\n",
      "iter:  796  loss:  -2.644874095916748\n",
      "iter:  797  loss:  -2.648481845855713\n",
      "iter:  798  loss:  -2.652092456817627\n",
      "iter:  799  loss:  -2.655704975128174\n",
      "iter:  800  loss:  -2.659320831298828\n",
      "iter:  801  loss:  -2.662937879562378\n",
      "iter:  802  loss:  -2.6665587425231934\n",
      "iter:  803  loss:  -2.6701817512512207\n",
      "iter:  804  loss:  -2.673807382583618\n",
      "iter:  805  loss:  -2.677435874938965\n",
      "iter:  806  loss:  -2.6810667514801025\n",
      "iter:  807  loss:  -2.6847000122070312\n",
      "iter:  808  loss:  -2.688336133956909\n",
      "iter:  809  loss:  -2.6919748783111572\n",
      "iter:  810  loss:  -2.6956164836883545\n",
      "iter:  811  loss:  -2.6992599964141846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  812  loss:  -2.702906847000122\n",
      "iter:  813  loss:  -2.7065556049346924\n",
      "iter:  814  loss:  -2.7102081775665283\n",
      "iter:  815  loss:  -2.713862419128418\n",
      "iter:  816  loss:  -2.717519521713257\n",
      "iter:  817  loss:  -2.721179723739624\n",
      "iter:  818  loss:  -2.7248423099517822\n",
      "iter:  819  loss:  -2.7285079956054688\n",
      "iter:  820  loss:  -2.732175827026367\n",
      "iter:  821  loss:  -2.735846757888794\n",
      "iter:  822  loss:  -2.739520311355591\n",
      "iter:  823  loss:  -2.7431960105895996\n",
      "iter:  824  loss:  -2.746875286102295\n",
      "iter:  825  loss:  -2.7505569458007812\n",
      "iter:  826  loss:  -2.754241704940796\n",
      "iter:  827  loss:  -2.7579288482666016\n",
      "iter:  828  loss:  -2.7616188526153564\n",
      "iter:  829  loss:  -2.7653112411499023\n",
      "iter:  830  loss:  -2.7690064907073975\n",
      "iter:  831  loss:  -2.772704601287842\n",
      "iter:  832  loss:  -2.7764053344726562\n",
      "iter:  833  loss:  -2.78010892868042\n",
      "iter:  834  loss:  -2.7838151454925537\n",
      "iter:  835  loss:  -2.7875239849090576\n",
      "iter:  836  loss:  -2.79123592376709\n",
      "iter:  837  loss:  -2.794950485229492\n",
      "iter:  838  loss:  -2.7986671924591064\n",
      "iter:  839  loss:  -2.8023874759674072\n",
      "iter:  840  loss:  -2.80610990524292\n",
      "iter:  841  loss:  -2.8098349571228027\n",
      "iter:  842  loss:  -2.8135626316070557\n",
      "iter:  843  loss:  -2.817293405532837\n",
      "iter:  844  loss:  -2.8210268020629883\n",
      "iter:  845  loss:  -2.8247623443603516\n",
      "iter:  846  loss:  -2.8285012245178223\n",
      "iter:  847  loss:  -2.8322417736053467\n",
      "iter:  848  loss:  -2.8359858989715576\n",
      "iter:  849  loss:  -2.8397321701049805\n",
      "iter:  850  loss:  -2.84348201751709\n",
      "iter:  851  loss:  -2.8472328186035156\n",
      "iter:  852  loss:  -2.850987195968628\n",
      "iter:  853  loss:  -2.854743480682373\n",
      "iter:  854  loss:  -2.8585031032562256\n",
      "iter:  855  loss:  -2.862264394760132\n",
      "iter:  856  loss:  -2.8660287857055664\n",
      "iter:  857  loss:  -2.869795799255371\n",
      "iter:  858  loss:  -2.8735644817352295\n",
      "iter:  859  loss:  -2.877336263656616\n",
      "iter:  860  loss:  -2.881110429763794\n",
      "iter:  861  loss:  -2.884887218475342\n",
      "iter:  862  loss:  -2.8886663913726807\n",
      "iter:  863  loss:  -2.8924477100372314\n",
      "iter:  864  loss:  -2.8962314128875732\n",
      "iter:  865  loss:  -2.900017499923706\n",
      "iter:  866  loss:  -2.90380597114563\n",
      "iter:  867  loss:  -2.9075963497161865\n",
      "iter:  868  loss:  -2.9113893508911133\n",
      "iter:  869  loss:  -2.91518497467041\n",
      "iter:  870  loss:  -2.91898250579834\n",
      "iter:  871  loss:  -2.9227824211120605\n",
      "iter:  872  loss:  -2.926584005355835\n",
      "iter:  873  loss:  -2.9303882122039795\n",
      "iter:  874  loss:  -2.9341940879821777\n",
      "iter:  875  loss:  -2.938002347946167\n",
      "iter:  876  loss:  -2.941812753677368\n",
      "iter:  877  loss:  -2.9456253051757812\n",
      "iter:  878  loss:  -2.9494404792785645\n",
      "iter:  879  loss:  -2.953256845474243\n",
      "iter:  880  loss:  -2.957075834274292\n",
      "iter:  881  loss:  -2.9608969688415527\n",
      "iter:  882  loss:  -2.96471905708313\n",
      "iter:  883  loss:  -2.9685442447662354\n",
      "iter:  884  loss:  -2.9723708629608154\n",
      "iter:  885  loss:  -2.9761998653411865\n",
      "iter:  886  loss:  -2.9800307750701904\n",
      "iter:  887  loss:  -2.98386287689209\n",
      "iter:  888  loss:  -2.9876976013183594\n",
      "iter:  889  loss:  -2.9915342330932617\n",
      "iter:  890  loss:  -2.9953725337982178\n",
      "iter:  891  loss:  -2.9992127418518066\n",
      "iter:  892  loss:  -3.003054618835449\n",
      "iter:  893  loss:  -3.0068984031677246\n",
      "iter:  894  loss:  -3.010744094848633\n",
      "iter:  895  loss:  -3.014591932296753\n",
      "iter:  896  loss:  -3.018441915512085\n",
      "iter:  897  loss:  -3.0222926139831543\n",
      "iter:  898  loss:  -3.0261449813842773\n",
      "iter:  899  loss:  -3.0300002098083496\n",
      "iter:  900  loss:  -3.0338566303253174\n",
      "iter:  901  loss:  -3.037715196609497\n",
      "iter:  902  loss:  -3.0415749549865723\n",
      "iter:  903  loss:  -3.045436382293701\n",
      "iter:  904  loss:  -3.049299716949463\n",
      "iter:  905  loss:  -3.0531647205352783\n",
      "iter:  906  loss:  -3.0570311546325684\n",
      "iter:  907  loss:  -3.0609004497528076\n",
      "iter:  908  loss:  -3.0647709369659424\n",
      "iter:  909  loss:  -3.0686426162719727\n",
      "iter:  910  loss:  -3.072516679763794\n",
      "iter:  911  loss:  -3.076392889022827\n",
      "iter:  912  loss:  -3.0802695751190186\n",
      "iter:  913  loss:  -3.084148645401001\n",
      "iter:  914  loss:  -3.088029384613037\n",
      "iter:  915  loss:  -3.091911554336548\n",
      "iter:  916  loss:  -3.0957953929901123\n",
      "iter:  917  loss:  -3.0996816158294678\n",
      "iter:  918  loss:  -3.1035690307617188\n",
      "iter:  919  loss:  -3.1074581146240234\n",
      "iter:  920  loss:  -3.111348867416382\n",
      "iter:  921  loss:  -3.115241765975952\n",
      "iter:  922  loss:  -3.119136333465576\n",
      "iter:  923  loss:  -3.123032331466675\n",
      "iter:  924  loss:  -3.1269302368164062\n",
      "iter:  925  loss:  -3.1308302879333496\n",
      "iter:  926  loss:  -3.1347320079803467\n",
      "iter:  927  loss:  -3.1386353969573975\n",
      "iter:  928  loss:  -3.142540693283081\n",
      "iter:  929  loss:  -3.14644718170166\n",
      "iter:  930  loss:  -3.1503562927246094\n",
      "iter:  931  loss:  -3.154266834259033\n",
      "iter:  932  loss:  -3.158179521560669\n",
      "iter:  933  loss:  -3.1620941162109375\n",
      "iter:  934  loss:  -3.1660099029541016\n",
      "iter:  935  loss:  -3.169928550720215\n",
      "iter:  936  loss:  -3.173848867416382\n",
      "iter:  937  loss:  -3.1777706146240234\n",
      "iter:  938  loss:  -3.181694984436035\n",
      "iter:  939  loss:  -3.1856212615966797\n",
      "iter:  940  loss:  -3.189549207687378\n",
      "iter:  941  loss:  -3.193479299545288\n",
      "iter:  942  loss:  -3.19741153717041\n",
      "iter:  943  loss:  -3.2013461589813232\n",
      "iter:  944  loss:  -3.205282211303711\n",
      "iter:  945  loss:  -3.2092208862304688\n",
      "iter:  946  loss:  -3.213162422180176\n",
      "iter:  947  loss:  -3.2171053886413574\n",
      "iter:  948  loss:  -3.2210512161254883\n",
      "iter:  949  loss:  -3.224999189376831\n",
      "iter:  950  loss:  -3.2289490699768066\n",
      "iter:  951  loss:  -3.2329015731811523\n",
      "iter:  952  loss:  -3.23685622215271\n",
      "iter:  953  loss:  -3.240814208984375\n",
      "iter:  954  loss:  -3.244774103164673\n",
      "iter:  955  loss:  -3.2487363815307617\n",
      "iter:  956  loss:  -3.252701759338379\n",
      "iter:  957  loss:  -3.256669759750366\n",
      "iter:  958  loss:  -3.2606406211853027\n",
      "iter:  959  loss:  -3.2646143436431885\n",
      "iter:  960  loss:  -3.2685909271240234\n",
      "iter:  961  loss:  -3.2725698947906494\n",
      "iter:  962  loss:  -3.27655291557312\n",
      "iter:  963  loss:  -3.28053879737854\n",
      "iter:  964  loss:  -3.2845280170440674\n",
      "iter:  965  loss:  -3.288520097732544\n",
      "iter:  966  loss:  -3.292515993118286\n",
      "iter:  967  loss:  -3.2965149879455566\n",
      "iter:  968  loss:  -3.3005173206329346\n",
      "iter:  969  loss:  -3.3045244216918945\n",
      "iter:  970  loss:  -3.3085341453552246\n",
      "iter:  971  loss:  -3.3125486373901367\n",
      "iter:  972  loss:  -3.3165674209594727\n",
      "iter:  973  loss:  -3.3205902576446533\n",
      "iter:  974  loss:  -3.324617385864258\n",
      "iter:  975  loss:  -3.328648090362549\n",
      "iter:  976  loss:  -3.3326847553253174\n",
      "iter:  977  loss:  -3.3367254734039307\n",
      "iter:  978  loss:  -3.3407723903656006\n",
      "iter:  979  loss:  -3.3448238372802734\n",
      "iter:  980  loss:  -3.348881244659424\n",
      "iter:  981  loss:  -3.352943181991577\n",
      "iter:  982  loss:  -3.3570120334625244\n",
      "iter:  983  loss:  -3.3610870838165283\n",
      "iter:  984  loss:  -3.365168809890747\n",
      "iter:  985  loss:  -3.369256019592285\n",
      "iter:  986  loss:  -3.3733508586883545\n",
      "iter:  987  loss:  -3.377453327178955\n",
      "iter:  988  loss:  -3.381563186645508\n",
      "iter:  989  loss:  -3.38568115234375\n",
      "iter:  990  loss:  -3.3898067474365234\n",
      "iter:  991  loss:  -3.393941640853882\n",
      "iter:  992  loss:  -3.398085594177246\n",
      "iter:  993  loss:  -3.402238607406616\n",
      "iter:  994  loss:  -3.4064013957977295\n",
      "iter:  995  loss:  -3.4105746746063232\n",
      "iter:  996  loss:  -3.41475772857666\n",
      "iter:  997  loss:  -3.4189529418945312\n",
      "iter:  998  loss:  -3.423159122467041\n",
      "iter:  999  loss:  -3.427377462387085\n",
      "iter:  1000  loss:  -3.431608200073242\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(Xb_train)\n",
    "y = torch.FloatTensor(yb_train)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1))\n",
    "    print('iter: ', i+1,' loss: ', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.softmax(pred, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель для данных с многими классами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm, ym = multy_data.values[:, :-1], multy_data.values[:, -1]\n",
    "ym = np.where(ym == 3, 0, ym)\n",
    "Xm_train, Xm_test, ym_train, ym_test = Xm[:-5, :], Xm[-5:, :], ym[:-5], ym[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(Xm_train)\n",
    "y = torch.FloatTensor(ym_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultyNeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MultyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(2, 3)\n",
    "        self.layer_2 = torch.nn.Linear(3, 4)\n",
    "        self.layer_out = torch.nn.Linear(4, 3)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_1 = self.sigmoid(self.layer_1(inputs))\n",
    "        output_2 = self.sigmoid(self.layer_1(output_1))\n",
    "        output = self.layer_out(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultyNeuralNetwork(\n",
      "  (layer_1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (layer_2): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (layer_out): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultyNeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (995x2 and 3x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-7f0b747d933b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ski6a\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (995x2 and 3x10)"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
